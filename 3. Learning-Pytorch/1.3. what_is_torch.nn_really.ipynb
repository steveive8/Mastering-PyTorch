{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('dl': conda)"
  },
  "interpreter": {
   "hash": "cf9fc2dbd5635ad5c962111ac36bfe3c6f06dcc7ef17370686f0520d53cd60ca"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Mastering PyTorch\n",
    "\n",
    "Edited by Steve Ive\n",
    "\n",
    "This document was edited to understand better for beginner of torch and basic concepts of deep learning base on original document below.\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/nn_tutorial.html\n",
    "\n",
    "# WHAT IS TORCH.NN REALLY?\n",
    "by Jeremy Howard, fast.ai. Thanks to Rachel Thomas and Francisco Ingham.\n",
    "\n",
    "We recommend running this tutorial as a notebook, not a script. To download the notebook (.ipynb) file, click the link at the top of the page.\n",
    "\n",
    "PyTorch provides the elegantly designed modules and classes torch.nn , torch.optim , Dataset , and DataLoader to help you create and train neural networks. In order to fully utilize their power and customize them for your problem, you need to really understand exactly what they’re doing. To develop this understanding, we will first train basic neural net on the MNIST data set without using any features from these models; we will initially only use the most basic PyTorch tensor functionality. Then, we will incrementally add one feature from torch.nn, torch.optim, Dataset, or DataLoader at a time, showing exactly what each piece does, and how it works to make the code either more concise, or more flexible.\n",
    "\n",
    "**This tutorial assumes you already have PyTorch installed, and are familiar with the basics of tensor operations.** (If you’re familiar with Numpy array operations, you’ll find the PyTorch tensor operations used here nearly identical).\n",
    "\n",
    "MNIST data setup\n",
    "We will use the classic MNIST dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
    "\n",
    "We will use pathlib for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests. We will only import modules when we use them, so you can see exactly what’s being used at each point."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# MNIST data setup\n",
    "\n",
    "We will use the classic MNIST dataset, which consists of black-and-white images of hand-drawn digits (between 0 and 9).\n",
    "\n",
    "We will use pathlib for dealing with paths (part of the Python 3 standard library), and will download the dataset using requests. We will only import modules when we use them, so you can see exactly what’s being used at each point."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n",
    "\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "if not (PATH / FILENAME).exists():\n",
    "    content = requests.get(URL + FILENAME).content\n",
    "    (PATH / FILENAME).open(\"wb\").write(content)"
   ]
  },
  {
   "source": [
    "This dataset is in numpy array format, and has been stored using pickle, a python-specific format for serializing data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), 'rb') as f:\n",
    "    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")"
   ]
  },
  {
   "source": [
    "Each image is 28 x 28, and is being stored as a flattened row of length 784 (=28x28). Let’s take a look at one; we need to reshape it to 2d first."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "\n",
    "pyplot.imshow(x_train[0].reshape((28, 28)), cmap=\"gray\")\n",
    "print(x_train.shape)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 389,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 784)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-07-16T17:41:50.843400</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 251.565 248.518125 \r\nL 251.565 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\nL 244.365 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g clip-path=\"url(#pf4e4ee939b)\">\r\n    <image height=\"218\" id=\"image0b4f1c497e\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\r\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\" y=\"-6.64\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mfd21891a5c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#mfd21891a5c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc2346338e8\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"11.082857\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"49.911429\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"88.74\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"127.568571\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"166.397143\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mc2346338e8\" y=\"205.225714\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 244.365 224.64 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 244.365 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 244.365 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf4e4ee939b\">\r\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "source": [
    "PyTorch uses torch.tensor, rather than numpy arrays, so we need to convert our data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "### Python map() function\n",
    "\n",
    "```map()``` function returns a map object(which is an iterator) of the results after applying the given function to each item of a given iterable (list, tuple etc.)\n",
    "\n",
    "**Syntax** :\n",
    "\n",
    "> ```map(fun, iter)```\n",
    "\n",
    "**Parameters** :\n",
    "\n",
    "> fun : It is a function to which map passes each element of given iterable.\n",
    "\n",
    "> iter : It is a iterable which is to be mapped.\n",
    "\n",
    "**NOTE** : You can pass one or more iterable to the map() function.\n",
    "\n",
    "**Returns** :\n",
    "\n",
    "> Returns a list of the results after applying the given function  \n",
    "to each item of a given iterable (list, tuple etc.) \n",
    " \n",
    "**NOTE** : The returned value from map() (map object) then can be passed to functions like list() (to create a list), set() (to create a set) .\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment! \n",
    "### ```torch.tensor()``` function\n",
    "\n",
    "> ```torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False)``` → Tensor\n",
    "\n",
    "Constructs a tensor with data.\n",
    "\n",
    "**Parameters**\n",
    "> **data (```array_like```)** – Initial data for the tensor. Can be a list, tuple, NumPy ndarray, scalar, and other types.\n",
    "\n",
    "**Keyword Arguments**\n",
    "- **dtype (```torch.dtype```, optional)** – the desired data type of returned tensor. Default: ***if None, infers data type from data.***\n",
    "\n",
    "- **device** (```torch.device```, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
    "\n",
    "- **requires_grad** (bool, optional) – If autograd should record operations on the returned tensor. Default: False.\n",
    "\n",
    "- **pin_memory** (bool, optional) – If set, returned tensor would be allocated in the pinned memory. Works only for CPU tensors. Default: False.\n",
    "\n",
    "---\n",
    "\n",
    "### TORCH.TENSOR datatype\n",
    "A ```torch.Tensor``` is a multi-dimensional matrix containing elements of a single **data type**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])\n",
      "torch.Size([50000, 784])\n",
      "tensor(0.) tensor(9)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x_train, y_train, x_valid, y_valid = map(\n",
    "    torch.tensor, (x_train, y_train, x_valid, y_valid)\n",
    ")\n",
    "n, c = x_train.shape\n",
    "print(x_train, y_train)\n",
    "print(x_train.shape)\n",
    "print(x_train.min(), y_train.max())"
   ]
  },
  {
   "source": [
    "# Neural net from scratch (no torch.nn)\n",
    "Let’s first create a model using nothing but PyTorch tensor operations. We’re assuming you’re already familiar with the basics of neural networks. (If you’re not, you can learn them at course.fast.ai).\n",
    "\n",
    "PyTorch provides methods to create random or zero-filled tensors, which we will use to create our weights and bias for a simple linear model. These are just regular tensors, with one very special addition: we tell PyTorch that they require a gradient. This causes PyTorch to record all of the operations done on the tensor, so that it can calculate the gradient during back-propagation automatically!\n",
    "\n",
    "For the weights, we set requires_grad **after** the initialization, since we don’t want that step included in the gradient. (Note that a trailing _ in PyTorch signifies that the operation is performed in-place.)\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "We are initializing the weights here with Xavier initialisation (by multiplying with 1/sqrt(n)).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "### TORCH.TENSOR.REQUIRES_GRAD_\n",
    "\n",
    "```Tensor.requires_grad_```(requires_grad=True) → Tensor\n",
    "\n",
    "Change if autograd should record operations on this tensor: sets this tensor’s requires_grad attribute in-place. Returns this tensor.\n",
    "\n",
    "requires_grad_()’s main use case is to tell autograd to begin recording operations on a Tensor tensor. If tensor has requires_grad=False (because it was obtained through a DataLoader, or required preprocessing or initialization), tensor.requires_grad_() makes it so that autograd will begin to record operations on tensor.\n",
    "\n",
    "Parameters\n",
    "**requires_grad** (bool) – If autograd should record operations on this tensor. Default: True.\n",
    "\n",
    "Example:\n",
    "\n",
    "> ```>>> # Let's say we want to preprocess some saved weights and use\n",
    ">>> # the result as new weights.\n",
    ">>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
    ">>> loaded_weights = torch.tensor(saved_weights)\n",
    ">>> weights = preprocess(loaded_weights)  # some function\n",
    ">>> weights\n",
    "tensor([-0.5503,  0.4926, -2.1158, -0.8303])```\n",
    "\n",
    "> ```>>> # Now, start to record operations done to weights\n",
    ">>> weights.requires_grad_()\n",
    ">>> out = weights.pow(2).sum()\n",
    ">>> out.backward()\n",
    ">>> weights.grad\n",
    "tensor([-1.1007,  0.9853, -4.2316, -1.6606])```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "source": [
    "Thanks to PyTorch’s ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model! So let’s just write a plain matrix multiplication and broadcasted addition to create a simple linear model. We also need an activation function, so we’ll write log_softmax and use it. Remember: although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
    "\n",
    "def model(xb):\n",
    "    return log_softmax(xb @ weights + bias)"
   ]
  },
  {
   "source": [
    "In the above, the @ stands for the dot product operation. We will call our function on one batch of data (in this case, 64 images). This is one forward pass. Note that our predictions won’t be any better than random at this stage, since we start with random weights."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([-2.8171, -2.7089, -2.0810, -1.6754, -2.0630, -2.7324, -2.1983, -2.3454,\n        -2.4395, -2.5846], grad_fn=<SelectBackward>) torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "bs = 64\n",
    "\n",
    "xb = x_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds[0], preds.shape\n",
    "print(preds[0], preds.shape)"
   ]
  },
  {
   "source": [
    "As you see, the preds tensor contains not only the tensor values, but also a gradient function. We’ll use this later to do backprop.\n",
    "\n",
    "Let’s implement negative log-likelihood to use as the loss function (again, we can just use standard Python):"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "```preds[range(target.shape[0]), target]```\n",
    "\n",
    "This is an indices itself to index of an specific matrix or a vector.\n",
    "\n",
    "so, if ```np.array([range(target.shape[0]), target])``` or just as a list like ```[range(target.shape[0]), target]``` will only show the itself, as\n",
    "\n",
    "```\n",
    "[range(0, 64), tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9, 4, 0, 9, 1,\n",
    "        1, 2, 4, 3, 2, 7, 3, 8, 6, 9, 0, 5, 6, 0, 7, 6, 1, 8, 7, 9, 3, 9, 8, 5,\n",
    "        9, 3, 3, 0, 7, 4, 9, 8, 0, 9, 4, 1, 4, 4, 6, 0])]```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(input, target):\n",
    "    return -input[range(target.shape[0]), target].mean()\n",
    "\n",
    "loss_func = nll"
   ]
  },
  {
   "source": [
    "Let’s check our loss with our random model, so we can see if we improve after a backprop pass later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3098, grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "yb = y_train[0:bs]\n",
    "print(loss_func(preds, yb))"
   ]
  },
  {
   "source": [
    "Let’s also implement a function to calculate the accuracy of our model. For each prediction, if the index with the largest value matches the target value, then the prediction was correct."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, yb):\n",
    "    preds = torch.argmax(out, dim=1)\n",
    "    return (preds == yb).float().mean()"
   ]
  },
  {
   "source": [
    "Let’s check the accuracy of our random model, so we can see if our accuracy improves as our loss improves.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.1094)\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(preds, yb))"
   ]
  },
  {
   "source": [
    "We can now run a training loop. For each iteration, we will:\n",
    "\n",
    "select a mini-batch of data (of size bs)\n",
    "use the model to make predictions\n",
    "calculate the loss\n",
    "loss.backward() updates the gradients of the model, in this case, weights and bias.\n",
    "We now use these gradients to update the weights and bias. We do this within the torch.no_grad() context manager, **because we do not want these actions to be recorded for our next calculation of the gradient.** You can read more about how PyTorch’s Autograd records operations here. https://pytorch.org/docs/stable/notes/autograd.html\n",
    "\n",
    "We then set the gradients to zero, so that we are ready for the next loop. Otherwise, our gradients would record a running tally of all the operations that had happened (i.e. loss.backward() adds the gradients to whatever is already stored, rather than replacing them).\n",
    "\n",
    "**TIP**\n",
    "\n",
    "You can use the standard python debugger to step through PyTorch code, allowing you to check the various variable values at each step. Uncomment set_trace() below to try it out."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "lr=0.5\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        # set_trace()\n",
    "        start_i = i *bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "        loss.backward() ## take backward() since loss return the torch.Tensor\n",
    "\n",
    "        with torch.no_grad():\n",
    "            weights -= weights.grad * lr\n",
    "            bias -= bias.grad * lr\n",
    "            weights.grad.zero_()\n",
    "            bias.grad.zero_()\n"
   ]
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "### NO_GRAD\n",
    "CLASS ```torch.no_grad```\n",
    "Context-manager that disabled gradient calculation.\n",
    "\n",
    "Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n",
    "\n",
    "In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\n",
    "\n",
    "This context manager is thread local; it will not affect computation in other threads.\n",
    "\n",
    "Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
    "\n",
    "\n",
    "### TORCH.TENSOR.BACKWARD\n",
    "\n",
    "```Tensor.backward(gradient=None, retain_graph=None, create_graph=False, inputs=None)```\n",
    "Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "\n",
    "The graph is differentiated using the chain rule. If the tensor is non-scalar (i.e. its data has more than one element) and requires gradient, the function additionally requires specifying gradient. It should be a tensor of matching type and location, that contains the gradient of the differentiated function w.r.t. self.\n",
    "\n",
    "This function accumulates gradients in the leaves - you might need to zero .grad attributes or set them to None before calling it. See Default gradient layouts for details on the memory layout of accumulated gradients.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "If you run any forward ops, create gradient, and/or call backward in a user-specified CUDA stream context, see Stream semantics of backward passes.\n",
    "\n",
    "#### Parameters\n",
    "- **gradient** (Tensor or None) – Gradient w.r.t. the tensor. If it is a tensor, it will be automatically converted to a Tensor that does not require grad unless create_graph is True. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable then this argument is optional.\n",
    "\n",
    "- **retain_graph** (bool, optional) – If False, the graph used to compute the grads will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.\n",
    "\n",
    "- **create_graph** (bool, optional) – If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False.\n",
    "\n",
    "- **inputs** (sequence of Tensor) – Inputs w.r.t. which the gradient will be accumulated into .grad. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors. All the provided inputs must be leaf Tensors."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "That’s it: we’ve created and trained a minimal neural network (in this case, a logistic regression, since we have no hidden layers) entirely from scratch!\n",
    "\n",
    "Let’s check the loss and accuracy and compare those to what we got earlier. We expect that the loss will have decreased and accuracy to have increased, and they have."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "Why this is logit regression?\n",
    "\n",
    "--- add\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0808, grad_fn=<NegBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "source": [
    "## Using torch.nn.functional\n",
    "We will now refactor our code, so that it does the same thing as before, only we’ll start taking advantage of PyTorch’s nn classes to make it more concise and flexible. At each step from here, we should be making our code one or more of: shorter, more understandable, and/or more flexible.\n",
    "\n",
    "The first and easiest step is to make our code shorter by replacing our hand-written activation and loss functions with those from torch.nn.functional (which is generally imported into the namespace F by convention). This module contains all the functions in the torch.nn library (whereas other parts of the library contain classes). As well as a wide range of loss and activation functions, you’ll also find here some convenient functions for creating neural nets, such as pooling functions. (There are also functions for doing convolutions, linear layers, etc, but as we’ll see, these are usually better handled using other parts of the library.)\n",
    "\n",
    "If you’re using negative log likelihood loss and log softmax activation, then Pytorch provides **a single function F.cross_entropy that combines the two. So we can even remove the activation function from our model.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "loss_func = F.cross_entropy\n",
    "\n",
    "def model(xb):\n",
    "    return xb @ weights + bias"
   ]
  },
  {
   "source": [
    "Note that we no longer call ```log_softmax``` in the ```model``` function. Let's confirm that our loss and accuracy are the same as before:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "Before, we defined the **model** that the form of dot producted result in the log_softmax,\n",
    "and give it to the loss function that is the negative likelihood with the yb.\n",
    "\n",
    "However the ```cross_entropy``` above contains the log_softmax in it, as we can just define the model as ```xb @ weights + bias```."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0808, grad_fn=<NllLossBackward>) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb), accuracy(model(xb), yb))"
   ]
  },
  {
   "source": [
    "## Refactor using nn.Module\n",
    "\n",
    "Next up, we’ll use nn.Module and nn.Parameter, for a clearer and more concise training loop. We subclass nn.Module (which itself is a class and able to keep track of state). In this case, we want to create a class that holds our weights, bias, and method for the forward step. nn.Module has a number of attributes and methods (such as .parameters() and .zero_grad()) which we will be using.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "nn.Module (uppercase M) is a PyTorch specific concept, and is a class we’ll be using a lot. nn.Module is not to be confused with the Python concept of a (lowercase m) module, which is a file of Python code that can be imported.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from torch import nn\n",
    "\n",
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(784, 10) / math.sqrt(784))\n",
    "        self.bias = nn.Parameter(torch.zeros(10))\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return xb @ self.weights + self.bias"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 402,
   "outputs": []
  },
  {
   "source": [
    "Since we're now using an object instead of just using a function, we first have to instantiate our model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mnist_Logistic()"
   ]
  },
  {
   "source": [
    "Now we can calculate the loss in the same way as before. Note that ```nn.Module``` objects are used as if they are functions (i.e they are ***callable***), but behind the scenes Pytorch will call our ```forward``` method automatically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.4357, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "source": [
    "Previously for our training loop we had to update the values for each parameter by name, and manually zero out the grads for each parameter separately, like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "source": [
    "Now we can take advantage of model.parameters() and model.zero_grad() (which are both defined by PyTorch for nn.Module) to make those steps more concise and less prone to the error of forgetting some of our parameters, particularly if we had a more complicated model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     for p in model.parameters(): p -= p.grad * lr\n",
    "#     model.zero_grad()"
   ]
  },
  {
   "source": [
    "We’ll wrap our little training loop in a fit function so we can run it again later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for i in range((n - 1) // bs + 1):\n",
    "            start_i = i * bs\n",
    "            end_i = start_i + bs\n",
    "            xb = x_train[start_i: end_i]\n",
    "            yb = y_train[start_i: end_i]\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= p.grad * lr\n",
    "                model.zero_grad()\n",
    "\n",
    "fit()"
   ]
  },
  {
   "source": [
    "Let’s double-check that our loss has gone down:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "print(loss_func(model(xb), yb))\n"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 408,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0813, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Refactor using nn.Linear\n",
    "\n",
    "We continue to refactor our code. Instead of manually defining and initializing self.weights and self.bias, and calculating xb  @ self.weights + self.bias, we will instead use the Pytorch class nn.Linear for a linear layer, which does all that for us. Pytorch has many types of predefined layers that can greatly simplify our code, and often makes it faster too."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mnist_Logistic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "source": [
    "We instantiate our model and calculate the loss in the same way as before:\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.2738, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = Mnist_Logistic()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "source": [
    "We are still able to use our same fit method as before."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0805, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "fit()\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "source": [
    "## Refactor using optim\n",
    "\n",
    "Pytorch also has a package with various optimization algorithms, torch.optim. We can use the step method from our optimizer to take a forward step, instead of manually updating each parameter.\n",
    "\n",
    "This will let us replace our previous manually coded optimization step:\n",
    "\n",
    "```\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters(): p -= p.grad * lr\n",
    "    model.zero_grad()\n",
    "```\n",
    "\n",
    "and instead use just:\n",
    "\n",
    "```\n",
    "\n",
    "opt.step()\n",
    "opt.zero_grad()\n",
    "\n",
    "```\n",
    "\n",
    "(```optim.zero_grad()``` resets the gradient to 0 and we need to call it before computing the gradient for the next minbatch.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from torch import optim"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 412,
   "outputs": []
  },
  {
   "source": [
    "We'll define a little function to create our model and optimizer so we can reuse it in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(2.3691, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2291, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    model = Mnist_Logistic()\n",
    "    return model, optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "model, opt = get_model()\n",
    "\n",
    "x_train_set = x_train[0: bs]\n",
    "y_train_set = y_train[0: bs]\n",
    "print(loss_func(model(x_train_set), y_train_set))\n",
    "# print(loss_func(model(xb), yb)) => on tutorial => but the shape of xb is (16, 784) since the xb resigned by iteration of epochs\n",
    "# print(50000 % 64) === 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        start_i = i * bs\n",
    "        end_i = start_i + bs\n",
    "        xb = x_train[start_i:end_i]\n",
    "        yb = y_train[start_i:end_i]\n",
    "\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(x_train_set), y_train_set))\n",
    "\n",
    "# output will return similar loss every playback since we load the model all the time on the top."
   ]
  },
  {
   "source": [
    "## Refactor using Dataset\n",
    "\n",
    "PyTorch has an abstract Dataset ```class```. A Dataset can be anything that has a ```__len__``` function (called by Python’s standard ```len``` function) and a ```__getitem__``` function as a way of indexing into it. This tutorial walks through a nice example of creating a custom ```FacialLandmarkDataset``` class as a subclass of ```Dataset```.\n",
    "\n",
    "PyTorch’s TensorDataset is a Dataset wrapping tensors. By defining a length and way of indexing, this also gives us a way to iterate, index, and slice along the first dimension of a tensor. This will make it easier to access both the independent and dependent variables in the same line as we train."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "from torch.utils.data import TensorDataset"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 414,
   "outputs": []
  },
  {
   "source": [
    "Both x_train and y_train can be combined in a single TensorDataset, which will be easier to iterate over and slice.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)"
   ]
  },
  {
   "source": [
    "Previously, we had to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "Now, we can do these two steps together:\n",
    "\n",
    "```xb, yb = train_ds[i*bs + i*bs+bs]```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0826, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range((n - 1) // bs + 1):\n",
    "        xb, yb = train_ds[i * bs: i * bs + bs]\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb)) "
   ]
  },
  {
   "source": [
    "## Refactor using DataLoader\n",
    "\n",
    "Pytorch’s ```DataLoader``` is responsible for managing batches. You can create a ```DataLoader``` from any ```Dataset```. DataLoader makes it easier to iterate over batches. Rather than having to use ```train_ds[i*bs : i*bs+bs]```, the DataLoader gives us each minibatch automatically.\n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size = bs)"
   ]
  },
  {
   "source": [
    "Previously, our loop iterated over batches (xb, yb) like this:\n",
    "\n",
    "```\n",
    "for i in range((n-1)//bs + 1):\n",
    "    xb,yb = train_ds[i*bs : i*bs+bs]\n",
    "    pred = model(xb)\n",
    "```\n",
    "Now, our loop is much cleaner, as (xb, yb) are loaded automatically from the data loader:\n",
    "```\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "**Refactor Process**\n",
    "\n",
    "```\n",
    "xb = x_train[start_i:end_i]\n",
    "yb = y_train[start_i:end_i]\n",
    "```\n",
    "\n",
    "After using ```torch.utils.data.TensorDataset```\n",
    "\n",
    "```\n",
    "xb, yb = train_ds[i*bs + i*bs+bs]\n",
    "```\n",
    "\n",
    "after using ```torch.utils.data.DataLoader```\n",
    "\n",
    "```\n",
    "for xb,yb in train_dl:\n",
    "    pred = model(xb)\n",
    "```\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.0807, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "print(loss_func(model(xb), yb))"
   ]
  },
  {
   "source": [
    "Thanks to Pytorch’s nn.Module, nn.Parameter, Dataset, and DataLoader, our training loop is now dramatically smaller and easier to understand. Let’s now try to add the basic features necessary to create effective models in practice."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Add Validation\n",
    "\n",
    "In section 1, we were just trying to get a reasonable training loop set up for use on our training data. In reality, you always should also have a validation set, in order to identify if you are overfitting.\n",
    "\n",
    "Shuffling the training data is important to prevent correlation between batches and overfitting. On the other hand, the validation loss will be identical whether we shuffle the validation set or not. Since shuffling takes extra time, it makes no sense to shuffle the validation data.\n",
    "\n",
    "We’ll use a batch size for the validation set that is twice as large as that for the training set. This is because the validation set does not need backpropagation and thus takes less memory (it doesn’t need to store the gradients). We take advantage of this to use a larger batch size and compute the loss more quickly."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size = bs, shuffle= True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid)\n",
    "valid_dl = DataLoader(valid_ds, batch_size= bs* 2)"
   ]
  },
  {
   "source": [
    "We will calculate and print the validation loss at the end of each epoch.\n",
    "\n",
    "(Note that we always call model.train() before training, and model.eval() before inference, because these are used by layers such as nn.BatchNorm2d and nn.Dropout to ensure appropriate behaviour for these different phases.)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 tensor(0.3961)\n",
      "1 tensor(0.3538)\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for xb, yb in train_dl:\n",
    "        pred = model(xb)\n",
    "        loss = loss_func(pred, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = sum(loss_func(model(xb), yb) for xb, yb in valid_dl)\n",
    "\n",
    "    print(epoch, valid_loss / len(valid_dl))"
   ]
  },
  {
   "source": [
    "## Create fit() and get_data()\n",
    "\n",
    "We’ll now do a little refactoring of our own. Since we go through a similar process twice of calculating the loss for both the training set and the validation set, let’s make that into its own function, loss_batch, which computes the loss for one batch.\n",
    "\n",
    "We pass an optimizer in for the training set, and use it to perform backprop. For the validation set, we don’t pass an optimizer, so the method doesn’t perform backprop."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        \n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)"
   ]
  },
  {
   "source": [
    "```fit``` runs the necessary operations to train our model and compute the training and validation losses for each epoch.\n",
    "\n",
    "### Take a Moment\n",
    "\n",
    "```loss.item()```\n",
    "\n",
    "since loss is a tuple of tensor(loss_value, grad_fn), write ```loss.item()``` only returns the value of loss.\n",
    "\n",
    "i.e)\n",
    "loss : tensor(0.3109, grad_fn=<NllLossBackward>) loss.item() : 0.31085121631622314\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Take a Moment!\n",
    "\n",
    "**List generator comprehension**\n",
    "\n",
    "if just write ```func for in``` without any wraps like ```loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl```\n",
    "\n",
    "the error occurs\n",
    "```\n",
    "File \"<ipython-input-332-9c7cab139303>\", line 12\n",
    "    print('just', loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl)\n",
    "                  ^\n",
    "SyntaxError: Generator expression must be parenthesized\n",
    "```\n",
    "**StackOverflow**\n",
    "\n",
    "\"It is in the form of a list or generator comprehension, but without the wrapping [] for the list or () for the generator.\" https://stackoverflow.com/questions/61945870/generator-expression-must-be-parenthesized-on-a-constraint\n",
    "\n",
    "---\n",
    "\n",
    "** *[] Method**\n",
    "\n",
    "\n",
    "Using * in front of a list expands out the members as individual arguments. So, the following two function calls will be equivalent:\n",
    "```\n",
    "my_function(*[1, 2, 3])\n",
    "my_function(1, 2, 3)\n",
    "```\n",
    "Obviously, the first one is not very useful if you already know the precise number of arguments. It becomes more useful with a comprehension like you are using, where is is not clear how many items will be in the list.\n",
    "https://stackoverflow.com/questions/45825344/understanding-in-python-passed-to-agg-in-pyspark\n",
    "\n",
    "---\n",
    "**At Below**\n",
    "\n",
    "```\n",
    "print('without: ', (loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl))\n",
    "\n",
    "print()\n",
    "\n",
    "print('with: ', *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl])\n",
    "```\n",
    "with:  (0.36518675088882446, 128) (0.4869171977043152, 128) ... lots of items... (0.0817655399441719, 16)\n",
    "\n",
    "without:  <generator object fit.<locals>.<genexpr> at 0x000002CCC6FC9430>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\r\n",
    "    for epoch in range(epochs):\r\n",
    "        model.train()\r\n",
    "        for xb, yb in train_dl:\r\n",
    "            loss_batch(model, loss_func, xb, yb, opt)\r\n",
    "\r\n",
    "        model.eval()\r\n",
    "        with torch.no_grad():\r\n",
    "            losses, nums = zip(\r\n",
    "                *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\r\n",
    "            )\r\n",
    "        val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\r\n",
    "\r\n",
    "        print(epoch, val_loss)"
   ]
  },
  {
   "source": [
    "```get_data``` returns dataloaders for the training and validation seets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, valid_ds, bs):\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size = bs, shuffle=True),\n",
    "        DataLoader(valid_ds, batch_size = bs * 2)\n",
    "    )"
   ]
  },
  {
   "source": [
    "Now, out whole process of obtaining the data loaders and fitting the model can be run in 3 lines of code:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0.32186362125873563\n",
      "1 0.2855281656146049\n"
     ]
    }
   ],
   "source": [
    "train_dl, valid_dl = get_data(train_ds, valid_ds, bs)\n",
    "model, opt = get_model()\n",
    "fit(epochs, model, loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}